"""
Data Normalization Module for Duplicate Detection POC

This module provides comprehensive data normalization functions for partner data
to improve duplicate detection accuracy. The normalization follows a structured
approach:

1. Basic text normalization (uppercase, trim, unicode)
2. Domain-specific normalization (addresses, names, cities)
3. Date normalization
4. Final cleanup (remove spaces and special characters)

The module is designed to be resource-efficient and doesn't rely on external
ML models or hardcoded mappings.
"""

import pandas as pd
import re
import unicodedata


def normalize_text_basic(text: str) -> str:
    """
    Grundlegende Textnormalisierung ohne externe Modelle.

    Schritte:
    1. In Gro√übuchstaben umwandeln
    2. Trimmen
    3. Unicode-Normalisierung (deutsche Umlaute)

    Args:
        text (str): Zu normalisierender Text

    Returns:
        str: Normalisierter Text
    """
    if pd.isna(text) or text is None:
        return ""

    # 1. In Gro√übuchstaben umwandeln
    text = str(text).upper()

    # 2. Trimmen
    text = text.strip()

    # 3. Unicode-Normalisierung f√ºr deutsche Zeichen
    text = unicodedata.normalize("NFD", text)

    # Deutsche Umlaute manuell behandeln (h√§ufigste F√§lle)
    # Das ist ressourcenschonend und deckt die wichtigsten deutschen Zeichen ab
    replacements = {
        "√Ñ": "AE",
        "√ñ": "OE",
        "√ú": "UE",
        "√ü": "SS",
        "√Ä": "A",
        "√Å": "A",
        "√Ç": "A",
        "√É": "A",
        "√à": "E",
        "√â": "E",
        "√ä": "E",
        "√ã": "E",
        "√å": "I",
        "√ç": "I",
        "√é": "I",
        "√è": "I",
        "√í": "O",
        "√ì": "O",
        "√î": "O",
        "√ï": "O",
        "√ô": "U",
        "√ö": "U",
        "√õ": "U",
        "√á": "C",
        "√ë": "N",
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    return text


def normalize_address(address: str) -> str:
    """
    Spezielle Normalisierung f√ºr Adressen.

    Behandelt h√§ufige Variationen in deutschen Adressen:
    - STR./STRA√üE -> STRASSE
    - Andere Abk√ºrzungen (PL. -> PLATZ, etc.)

    Args:
        address (str): Adresse

    Returns:
        str: Normalisierte Adresse
    """
    if pd.isna(address) or address is None:
        return ""

    address = normalize_text_basic(address)

    # 4. STR. und STRA√üE normalisieren
    address = re.sub(r"\bSTR\b\.?", "STRASSE", address)
    address = re.sub(r"\bSTRASSE\b", "STRASSE", address)

    # Weitere h√§ufige Abk√ºrzungen in deutschen Adressen
    address_replacements = {
        r"\bPL\b\.?": "PLATZ",
        r"\bPLZ\b\.?": "PLATZ",
        r"\bALLE\b": "ALLEE",
        r"\bDAMM\b": "DAMM",
        r"\bWEG\b": "WEG",
        r"\bGASSE\b": "GASSE",
        r"\bRING\b": "RING",
        r"\bUFER\b": "UFER",
        r"\bBRUECKE\b": "BRUECKE",
        r"\bTOR\b": "TOR",
        r"\bHOF\b": "HOF",
        r"\bNR\b\.?": "",  # Hausnummer-Kennzeichnung entfernen
        r"\bNUMMER\b": "",
    }

    for pattern, replacement in address_replacements.items():
        address = re.sub(pattern, replacement, address)

    return address


def normalize_name(name: str) -> str:
    """
    Spezielle Normalisierung f√ºr Namen (Vor- und Nachnamen).

    Verwendet phonetische √Ñhnlichkeitsmuster ohne externe Modelle.
    Behandelt h√§ufige Variationen in deutschen Namen.

    Args:
        name (str): Name

    Returns:
        str: Normalisierter Name
    """
    if pd.isna(name) or name is None:
        return ""

    name = normalize_text_basic(name)

    # H√§ufige phonetische Variationen von Namen normalisieren
    # Basiert auf h√§ufigen deutschen Schreibweisen und Aussprachen
    name_patterns = {
        r"\bCH\b": "K",  # Christian -> Kristian
        r"\bPH\b": "F",  # Philipp -> Filip
        r"\bTH\b": "T",  # Thomas -> Tomas
        r"\bCK\b": "K",  # Dirck -> Dirk
        r"\bQU\b": "KW",  # Quelle -> Kwelle
        r"\bX\b": "KS",  # Alexander -> Aleksander
        r"\bZ\b": "S",  # Franz -> Frans (in einigen Dialekten)
        r"\bY\b": "I",  # Yvonne -> Ivonne
        r"\bV\b": "F",  # Veit -> Feit (phonetisch)
        r"\bW\b": "V",  # Wilhelm -> Vilhelm
    }

    for pattern, replacement in name_patterns.items():
        name = re.sub(pattern, replacement, name)

    # Doppelte Konsonanten reduzieren (h√§ufig bei Namen)
    name = re.sub(r"([BCDFGHJKLMNPQRSTVWXZ])\1+", r"\1", name)

    return name


def normalize_city(city: str) -> str:
    """
    Spezielle Normalisierung f√ºr Ortsnamen.

    Behandelt h√§ufige Variationen in deutschen Ortsnamen.

    Args:
        city (str): Ortsname

    Returns:
        str: Normalisierter Ortsname
    """
    if pd.isna(city) or city is None:
        return ""

    city = normalize_text_basic(city)

    # H√§ufige Variationen in deutschen Ortsnamen
    city_patterns = {
        r"\bAM\b": "A",  # Frankfurt am Main -> Frankfurt A Main
        r"\bIM\b": "I",  # Weiden in der Oberpfalz
        r"\bBEI\b": "B",  # Neustadt bei Coburg
        r"\bAN\b": "A",  # Rothenburg an der Tauber
        r"\bAUF\b": "A",  # Roth auf der Roth
        r"\bINS\b": "I",  #
        r"\bUNTER\b": "U",  # Bad Reichenhall unter
        r"\bOBER\b": "O",  # Oberammergau
        r"\bNIEDER\b": "N",  # Niederbrechen
        r"\bGROSS\b": "G",  # Grossbottwar
        r"\bKLEIN\b": "K",  # Kleinmachnow
        r"\bSANKT\b": "ST",  # Sankt Augustin -> St Augustin
        r"\bST\b\.?": "ST",  # St. -> ST
        r"\bBAD\b": "B",  # Bad Homburg -> B Homburg
    }

    for pattern, replacement in city_patterns.items():
        city = re.sub(pattern, replacement, city)

    return city


def normalize_date(date_str: str) -> str:
    """
    Datumsnormalisierung - vereinheitlicht auf YYYY-MM-DD Format.
    Beh√§lt Sonderzeichen bei (Bindestriche).

    Args:
        date_str (str): Datum als String

    Returns:
        str: Normalisiertes Datum im Format YYYY-MM-DD
    """
    if pd.isna(date_str) or date_str is None:
        return ""

    # Nur Leerzeichen trimmen, Sonderzeichen beibehalten
    date_str = str(date_str).strip()

    # Versuche verschiedene Datumsformate zu parsen
    date_patterns = [
        r"(\d{4})-(\d{2})-(\d{2})",  # YYYY-MM-DD (bereits korrekt)
        r"(\d{2})\.(\d{2})\.(\d{4})",  # DD.MM.YYYY (deutsch)
        r"(\d{2})/(\d{2})/(\d{4})",  # DD/MM/YYYY
        r"(\d{4})(\d{2})(\d{2})",  # YYYYMMDD (ohne Trenner)
        r"(\d{1,2})\.(\d{1,2})\.(\d{4})",  # D.M.YYYY oder DD.MM.YYYY
        r"(\d{4})/(\d{2})/(\d{2})",  # YYYY/MM/DD
    ]

    for pattern in date_patterns:
        match = re.search(pattern, date_str)
        if match:
            groups = match.groups()
            if len(groups) == 3:
                # Bestimme Format basierend auf der ersten Gruppe
                if len(groups[0]) == 4:  # Jahr zuerst (YYYY-MM-DD, YYYY/MM/DD)
                    year, month, day = groups
                else:  # Tag zuerst (DD.MM.YYYY, DD/MM/YYYY)
                    day, month, year = groups

                # Validierung und Formatierung
                try:
                    day, month, year = int(day), int(month), int(year)

                    # Grundlegende Plausibilit√§tspr√ºfung
                    if 1 <= month <= 12 and 1 <= day <= 31 and 1900 <= year <= 2100:
                        return f"{year:04d}-{month:02d}-{day:02d}"

                except ValueError:
                    continue

    # Fallback: Original zur√ºckgeben wenn kein g√ºltiges Format erkannt
    return date_str


def remove_special_chars_and_spaces(text: str, preserve_date_chars: bool = False) -> str:
    """
    Entfernt Leerzeichen und Sonderzeichen (au√üer bei Datumsfeldern).

    Dies ist der finale Normalisierungsschritt.

    Args:
        text (str): Text
        preserve_date_chars (bool): Ob Datums-Sonderzeichen beibehalten werden sollen

    Returns:
        str: Bereinigter Text
    """
    if pd.isna(text) or text is None:
        return ""

    text = str(text)

    if preserve_date_chars:
        # Nur Leerzeichen entfernen, Datums-Sonderzeichen (Bindestriche) behalten
        text = re.sub(r"\s+", "", text)
    else:
        # Alle Leerzeichen entfernen
        text = re.sub(r"\s+", "", text)
        # Sonderzeichen entfernen (nur Buchstaben und Zahlen behalten)
        text = re.sub(r"[^A-Z0-9]", "", text)

    return text


def normalize_partner_data(
    df: pd.DataFrame,
    normalize_for_splink: bool = True,
    enhanced_mode: bool = False,
    phonetic_names: bool = False,
    fuzzy_cities: bool = False,
    nlp_addresses: bool = False,
) -> pd.DataFrame:
    """
    Vollst√§ndige Normalisierung der Partnerdaten mit optionalen Erweiterungen.

    F√ºhrt alle Normalisierungsschritte in der korrekten Reihenfolge durch:
    1. Grundlegende Normalisierung (Gro√ü, Trim, Unicode)
    2. Spezielle Normalisierung nach Spaltentyp (mit optionalen Erweiterungen)
    3. Datumsnormalisierung
    4. Finale Bereinigung (Leerzeichen und Sonderzeichen)

    Args:
        df (pd.DataFrame): DataFrame mit Partnerdaten
        normalize_for_splink (bool): Ob finale Bereinigung f√ºr Splink durchgef√ºhrt werden soll
        enhanced_mode (bool): Aktiviert alle erweiterten Algorithmen
        phonetic_names (bool): Phonetische Algorithmen f√ºr Namen
        fuzzy_cities (bool): Fuzzy-Matching f√ºr Ortsnamen
        nlp_addresses (bool): NLP-basierte Adresserweiterungen

    Returns:
        pd.DataFrame: Normalisiertes DataFrame
    """
    if df is None or df.empty:
        return df

    df_normalized = df.copy()

    # Enhanced mode aktiviert alle Erweiterungen
    if enhanced_mode:
        phonetic_names = True
        fuzzy_cities = True
        nlp_addresses = True

    print("Starte umfassende Datennormalisierung...")
    print(f"Anzahl Datens√§tze: {len(df_normalized)}")
    print(f"Spalten: {list(df_normalized.columns)}")

    if enhanced_mode:
        print("üöÄ Enhanced Mode aktiviert - verwende erweiterte Algorithmen")

    # Check optional dependencies
    optional_deps = {}
    if phonetic_names or fuzzy_cities:
        try:
            import jellyfish

            optional_deps["jellyfish"] = True
            print("‚úì jellyfish verf√ºgbar f√ºr phonetische/fuzzy Algorithmen")
        except ImportError:
            optional_deps["jellyfish"] = False
            print("‚ö† jellyfish nicht installiert - verwende Standard-Algorithmen")

    # Schritt 1-3: Spezielle Normalisierung nach Spaltentyp
    for column in df_normalized.columns:
        print(f"  Normalisiere Spalte: {column}")

        if column in ["NAME"]:
            if phonetic_names and optional_deps.get("jellyfish", False):
                df_normalized[column] = df_normalized[column].apply(lambda x: normalize_name_enhanced(x, use_phonetic=True))
            else:
                df_normalized[column] = df_normalized[column].apply(normalize_name)

        elif column in ["VORNAME"]:
            if phonetic_names and optional_deps.get("jellyfish", False):
                df_normalized[column] = df_normalized[column].apply(lambda x: normalize_name_enhanced(x, use_phonetic=True))
            else:
                df_normalized[column] = df_normalized[column].apply(normalize_name)

        elif column in ["ORT"]:
            if fuzzy_cities and optional_deps.get("jellyfish", False):
                df_normalized[column] = df_normalized[column].apply(lambda x: normalize_city_enhanced(x, fuzzy_matching=True))
            else:
                df_normalized[column] = df_normalized[column].apply(normalize_city)

        elif column in ["ADRESSZEILE"]:
            if nlp_addresses:
                df_normalized[column] = df_normalized[column].apply(lambda x: normalize_address_enhanced(x, use_nlp=True))
            else:
                df_normalized[column] = df_normalized[column].apply(normalize_address)

        elif column in ["GEBURTSDATUM"]:
            df_normalized[column] = df_normalized[column].apply(normalize_date)
        else:
            # Grundlegende Normalisierung f√ºr alle anderen Spalten
            df_normalized[column] = df_normalized[column].apply(normalize_text_basic)

    # Schritt 4: Finale Bereinigung f√ºr Splink (optional)
    if normalize_for_splink:
        print("  Finale Bereinigung: Entferne Leerzeichen und Sonderzeichen...")

        for column in df_normalized.columns:
            if column == "GEBURTSDATUM":
                # Bei Datum nur Leerzeichen entfernen, Bindestriche behalten
                df_normalized[column] = df_normalized[column].apply(
                    lambda x: remove_special_chars_and_spaces(x, preserve_date_chars=True)
                )
            else:
                # Bei allen anderen Spalten: Leerzeichen und Sonderzeichen entfernen
                df_normalized[column] = df_normalized[column].apply(
                    lambda x: remove_special_chars_and_spaces(x, preserve_date_chars=False)
                )

    print("Datennormalisierung erfolgreich abgeschlossen!")

    # Zeige Beispiele der Normalisierung
    if len(df_normalized) > 0:
        print("\nBeispiele der Normalisierung:")
        for column in ["NAME", "VORNAME", "ORT", "ADRESSZEILE"][:3]:
            if column in df_normalized.columns:
                original = df.iloc[0][column] if not pd.isna(df.iloc[0][column]) else "N/A"
                normalized = df_normalized.iloc[0][column] if not pd.isna(df_normalized.iloc[0][column]) else "N/A"
                print(f"  {column}: '{original}' -> '{normalized}'")

    return df_normalized


def get_normalization_statistics(df_original: pd.DataFrame, df_normalized: pd.DataFrame) -> dict:
    """
    Erstellt Statistiken √ºber die Normalisierung.

    Args:
        df_original (pd.DataFrame): Original DataFrame
        df_normalized (pd.DataFrame): Normalisiertes DataFrame

    Returns:
        dict: Statistiken √ºber die Normalisierung
    """
    stats = {"total_records": len(df_original), "columns_processed": len(df_original.columns), "changes_by_column": {}}

    for column in df_original.columns:
        if column in df_normalized.columns:
            original_values = df_original[column].astype(str)
            normalized_values = df_normalized[column].astype(str)

            changes = (original_values != normalized_values).sum()
            change_percentage = (changes / len(df_original)) * 100

            stats["changes_by_column"][column] = {"changes": int(changes), "change_percentage": round(change_percentage, 2)}

    return stats


# === ENHANCED HYBRID NORMALIZATION FUNCTIONS ===


def normalize_name_enhanced(name: str, use_phonetic: bool = False) -> str:
    """
    Erweiterte Namen-Normalisierung mit optionaler phonetischer Komponente.

    Kombiniert die bestehende regelbasierte Normalisierung mit optionalen
    phonetischen Algorithmen f√ºr bessere Duplikaterkennung.

    Args:
        name (str): Name
        use_phonetic (bool): Ob phonetische Algorithmen verwendet werden sollen

    Returns:
        str: Normalisierter Name
    """
    # Basis-Normalisierung (bereits vorhanden und bew√§hrt)
    name = normalize_name(name)

    # Optional: Phonetische Erweiterung
    if use_phonetic:
        try:
            import jellyfish

            # Soundex f√ºr deutsche Namen - robuster als Metaphone
            phonetic_code = jellyfish.soundex(name)
            # Kombiniere beide f√ºr beste Ergebnisse
            return f"{name}_{phonetic_code}" if phonetic_code else name
        except ImportError:
            # Graceful fallback - keine Abh√§ngigkeit erzwungen
            print("  Hinweis: jellyfish nicht installiert, verwende Standard-Normalisierung")

    return name


def normalize_city_enhanced(city: str, fuzzy_matching: bool = False) -> str:
    """
    Erweiterte Orts-Normalisierung mit optionalem Fuzzy-Matching.

    Erweitert die bestehende Ortsnormalisierung um Fuzzy-Matching gegen
    eine kleine Liste h√§ufiger deutscher St√§dte.

    Args:
        city (str): Ortsname
        fuzzy_matching (bool): Ob Fuzzy-Matching verwendet werden soll

    Returns:
        str: Normalisierter Ortsname
    """
    # Basis-Normalisierung (bereits vorhanden)
    city = normalize_city(city)

    # Optional: Fuzzy-Matching gegen bekannte deutsche St√§dte
    if fuzzy_matching:
        try:
            import jellyfish

            # Kleine Liste h√§ufiger deutscher St√§dte (nur Gro√üst√§dte)
            # Bewusst klein gehalten f√ºr Performance und Genauigkeit
            major_cities = {
                "BERLIN",
                "HAMBURG",
                "MUENCHEN",
                "KOELN",
                "FRANKFURT",
                "STUTTGART",
                "DUESSELDORF",
                "DORTMUND",
                "ESSEN",
                "LEIPZIG",
                "BREMEN",
                "DRESDEN",
                "HANNOVER",
                "NUERNBERG",
                "DUISBURG",
            }

            best_match = None
            best_score = 0

            for ref_city in major_cities:
                score = jellyfish.jaro_winkler_similarity(city, ref_city)
                if score > best_score and score > 0.85:  # Hoher Threshold f√ºr Genauigkeit
                    best_score = score
                    best_match = ref_city

            if best_match:
                # print(f"    Fuzzy-Match: '{city}' -> '{best_match}' (Score: {best_score:.2f})")
                return best_match

        except ImportError:
            print("  Hinweis: jellyfish nicht installiert, verwende Standard-Normalisierung")
        except Exception as e:
            print(f"  Hinweis: Fuzzy-Matching fehlgeschlagen ({e}), verwende Standard-Normalisierung")

    return city


def normalize_address_enhanced(address: str, use_nlp: bool = False) -> str:
    """
    Erweiterte Adress-Normalisierung mit optionaler NLP-Komponente.

    Args:
        address (str): Adresse
        use_nlp (bool): Ob NLP-basierte Erweiterungen verwendet werden sollen

    Returns:
        str: Normalisierte Adresse
    """
    # Basis-Normalisierung (bereits vorhanden und sehr gut)
    address = normalize_address(address)

    # Optional: NLP-basierte Erweiterungen f√ºr komplexere Adressen
    if use_nlp:
        try:
            # Hier k√∂nnte spaCy oder andere NLP-Tools integriert werden
            # F√ºr jetzt: Zus√§tzliche Regex-Patterns f√ºr komplexere F√§lle

            # Hausnummern normalisieren
            address = re.sub(r"(\d+)\s*([A-Z])\b", r"\1\2", address)  # "123 A" -> "123A"

            # Postf√§cher
            address = re.sub(r"\bPOSTFACH\b", "PF", address)
            address = re.sub(r"\bPF\b\.?", "PF", address)

        except Exception as e:
            print(f"  Hinweis: Erweiterte Adressnormalisierung fehlgeschlagen ({e})")

    return address
