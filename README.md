# Splink Duplikaterkennung - Production-Ready POC

Ein **modularer Proof of Concept** fÃ¼r Record Linkage und Duplikaterkennung mit deutschen Datenstrukturen. Das System kombiniert Splink v4, persistente DuckDB-Speicherung und umfassende Datennormalisierung zu einer produktionsreifen LÃ¶sung.

## ğŸš€ Quick Start

```bash
# Installation
uv sync

# Standardlauf mit Testdaten
uv run python -m src.dublette.app --generate-test-data

# Mit echter CSV-Datei und erweiterter Normalisierung
uv run python -m src.dublette.app --input-file output/partner_test.csv --enhanced-normalization

# Datenbankstatistiken anzeigen
uv run python -m src.dublette.app --show-db-stats
```

## âœ¨ Hauptfeatures

### ğŸ—ï¸ **Persistente DuckDB-Architektur**
- **Dauerhafte Speicherung**: Alle Daten in `output/splink_data.duckdb`
- **Intelligente Caching**: Zeitstempel-basierte Aktualisierung
- **Performance**: 70-90% schneller bei WiederholungslÃ¤ufen
- **Backup-freundlich**: Einfacher Export als CSV

### ğŸ‡©ï¿½ **Deutsche Datennormalisierung**
- **Standard-Mode**: Umlaute, StraÃŸenabkÃ¼rzungen, phonetische Regeln
- **Enhanced-Mode**: Soundex, Fuzzy-Matching, NLP-Adressen (mit jellyfish)
- **Backward-Compatible**: Funktioniert mit bestehenden Daten
- **Flexibel**: Ein-/ausschaltbar per CLI-Parameter

### ğŸ“Š **Umfassende Evaluation**
- **6-Plot-Analysis**: Wahrscheinlichkeitsverteilung, Threshold-SensitivitÃ¤t, QualitÃ¤tsindikatoren
- **Markdown-Bericht**: Automatisch generierter Evaluationsbericht
- **VerstÃ¤ndliche Metriken**: Deutsche ErklÃ¤rungen und Empfehlungen
- **Visualisierungen**: 4 verschiedene Plot-Sets fÃ¼r tiefe Einblicke

### âš¡ **Erweiterte Splink-Features**
- **Kombinierte Blocking Rules**: NAME+VORNAME, PLZ+ORT, phonetische Ã„hnlichkeit
- **Multi-Table & Single-Table**: Beide Modi vollstÃ¤ndig unterstÃ¼tzt
- **Deutsche Datenstrukturen**: Optimiert fÃ¼r SATZNR, NAME, VORNAME, etc.

## ğŸ”§ Verwendung

### **Produktive Workflows**

```bash
# ğŸ¯ Empfohlen: CSV mit Enhanced Normalization
uv run python -m src.dublette.app --input-file data.csv --enhanced-normalization

# âš¡ Performance: Bestehende Ergebnisse verwenden
uv run python -m src.dublette.app --use-existing-results

# ğŸ”„ Multi-Table Linking
uv run python -m src.dublette.app --multi-table --generate-test-data

# ğŸ“Š Nur Analyse ohne erneute Berechnung
uv run python -m src.dublette.app --show-db-stats
```

### **Development & Testing**

```bash
# Testdaten generieren
uv run python -m src.dublette.app --generate-test-data --enhanced-normalization

# Forcierte Aktualisierung
uv run python -m src.dublette.app --force-refresh

# Nur Normalisierung testen
uv run python -m src.dublette.app --normalize-existing

# Datenbank aufrÃ¤umen
uv run python -m src.dublette.app --cleanup-db
```

## ğŸ—‚ï¸ Ausgabefiles

```
output/
â”œâ”€â”€ splink_data.duckdb              # Persistente Hauptdatenbank
â”œâ”€â”€ evaluation_report.md            # Umfassender Evaluationsbericht
â”œâ”€â”€ comprehensive_evaluation_analysis.png
â”œâ”€â”€ detailed_threshold_analysis.png
â”œâ”€â”€ match_quality_heatmap.png
â”œâ”€â”€ match_probability_distribution.png
â”œâ”€â”€ predictions.csv                 # Optional (--save-csv-files)
â””â”€â”€ target_table.csv               # Optional (--save-csv-files)
```

## ğŸ›ï¸ Architektur

### **Module**
- `app.py`: CLI mit persistenter DuckDB-Integration
- `data/normalization.py`: Standard- und Enhanced-Normalisierung
- `database/connection.py`: DuckDB-Setup mit Caching
- `detection/splink_config.py`: Erweiterte Blocking Rules
- `evaluation/metrics.py`: Umfassende Evaluation und Berichte

### **Datenmodell (Deutsche Spaltennamen)**
```
SATZNR          # Eindeutige ID
NAME            # Nachname/Firmenname  
VORNAME         # Vorname
GEBURTSDATUM    # Datum (YYYY-MM-DD)
GESCHLECHT      # M/W
LAND            # LÃ¤ndercode
POSTLEITZAHL    # Deutsche PLZ
ORT             # Stadt
ADRESSZEILE     # VollstÃ¤ndige Adresse
```

## ğŸš€ Performance

### **Benchmark-Ergebnisse**
- **Erstes Laden**: Wie gewohnt (Setup-Zeit)
- **Wiederholte LÃ¤ufe**: 70-90% schneller durch DuckDB-Caching
- **Mit `--use-existing-results`**: 95% schneller
- **Enhanced Normalization**: 20-30% langsamer, deutlich bessere QualitÃ¤t

### **Speichereffizienz**
- **DuckDB vs CSV**: 50-70% weniger Speicherplatz
- **Komprimierung**: Automatische Optimierung
- **Cleanup**: Entfernt temporÃ¤re Tabellen

---

**Production-ready Duplikaterkennung mit deutscher Lokalisierung und persistenter DuckDB-Architektur! ğŸ‡©ğŸ‡ªâš¡**