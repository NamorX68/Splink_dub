# Duplicate Detection POC - Usage Guide

## √úberblick

Diese Anwendung f√ºhrt Duplikaterkennung mit Splink und DuckDB durch. Sie unterst√ºtzt sowohl **Eintabellen-Deduplication** als auch **Mehrtabellen-Linking**.

**NEU: Umfassende Datennormalisierung** f√ºr verbesserte Duplikaterkennung mit deutschen Datenstrukturen.

## Installation

Verwenden Sie uv f√ºr die Installation der Dependencies:

```bash
uv sync
```

## Aktuelle Normalisierungsfeatures

### Automatische Datennormalisierung

Das System normalisiert die Daten automatisch in folgenden Schritten:

1. **Grundnormalisierung**: Gro√übuchstaben, Trimmen, Unicode (√§‚ÜíAE, √∂‚ÜíOE, √º‚ÜíUE, √ü‚ÜíSS)
2. **Adressnormalisierung**: STR./STRA√üE ‚Üí STRASSE, PL. ‚Üí PLATZ
3. **Namennormalisierung**: Phonetische √Ñhnlichkeit (CH‚ÜíK, PH‚ÜíF, TH‚ÜíT)
4. **Ortsnormalisierung**: "Frankfurt am Main" ‚Üí "Frankfurt A Main"
5. **Datumsnormalisierung**: Verschiedene Formate ‚Üí YYYY-MM-DD
6. **Finale Bereinigung**: Leerzeichen und Sonderzeichen entfernen

### Erweiterte Normalisierung (üöÄ Enhanced Mode)

Mit dem `--enhanced-normalization` Flag werden zus√§tzliche Algorithmen aktiviert:

- **Phonetische Namen**: Soundex-Codes f√ºr √§hnlich klingende Namen (ben√∂tigt jellyfish)
- **Fuzzy-Matching f√ºr St√§dte**: Automatische Korrektur gegen deutsche Gro√üst√§dte
- **NLP-basierte Adressen**: Erweiterte Adressnormalisierung mit komplexeren Mustern

## Verwendung

### CLI-Parameter

Die Anwendung unterst√ºtzt folgende Parameter:

- `--multi-table`: Aktiviert Mehrtabellen-Verarbeitung (Standard: False = Eintabellen-Deduplication)
- `--generate-test-data`: Generiert neue Testdaten (Standard: False = verwendet existierende Daten)
- `--table-name TEXT`: Name der Tabelle f√ºr Eintabellen-Verarbeitung (Standard: 'company_data')
- `--input-file PATH`: Pfad zu einer CSV-Datei mit Partnerdaten
- `--normalize-data`: Datennormalisierung anwenden (Standard: True)
- `--enhanced-normalization`: üöÄ Erweiterte Normalisierung aktivieren (ben√∂tigt jellyfish)
- `--normalize-existing`: Nur bestehende Daten normalisieren, ohne Duplikaterkennung

### Empfohlene Kommandos

```bash
# üöÄ Mit partner_test.csv und erweiterter Normalisierung (empfohlen)
uv run python -m src.dublette.app --input-file output/partner_test.csv --enhanced-normalization

# Standard-Normalisierung mit partner_test.csv
uv run python -m src.dublette.app --input-file output/partner_test.csv

# Nur bestehende Daten normalisieren
uv run python -m src.dublette.app --normalize-existing

# Eigene CSV-Datei mit erweiterter Normalisierung
uv run python -m src.dublette.app --input-file meine_daten.csv --enhanced-normalization

# Datenbankstatistiken anzeigen
uv run python -m src.dublette.app --show-db-stats

# Alle Tabellen als CSV exportieren
uv run python -m src.dublette.app --export-db-to-csv
```

### Weitere Beispiele

#### 1. Eintabellen-Deduplication mit neuen Testdaten

```bash
uv run python -m src.dublette.app --generate-test-data
```

#### 2. Mehrtabellen-Linking mit neuen Testdaten

```bash
uv run python -m src.dublette.app --multi-table --generate-test-data
```

#### 3. Eintabellen-Deduplication mit existierenden Daten

```bash
uv run python -m src.dublette.app --table-name my_company_data
```

#### 4. Erweiterte Normalisierung mit Multi-Table

```bash
uv run python -m src.dublette.app --multi-table --input-file output/partner_test.csv --enhanced-normalization
```

#### 5. Nur Normalisierung ohne Duplikaterkennung

```bash
uv run python -m src.dublette.app --normalize-existing
```

#### 6. Persistente DuckDB-Funktionen

```bash
# Datenbankstatistiken anzeigen
uv run python -m src.dublette.app --show-db-stats

# Alle Tabellen als CSV exportieren
uv run python -m src.dublette.app --export-db-to-csv

# Datenbank aufr√§umen und optimieren
uv run python -m src.dublette.app --cleanup-db

# Bestehende Ergebnisse wiederverwenden
uv run python -m src.dublette.app --use-existing-results
```

#### 7. Hilfe anzeigen

```bash
uv run python -m src.dublette.app --help
```

## Modi im Detail

### Eintabellen-Deduplication (Standard)

- Erkennt Duplikate **innerhalb einer einzigen Tabelle**
- Verwendet `company_data` als Standard-Tabellenname
- Findet Datens√§tze, die sich selbst √§hneln

### Mehrtabellen-Linking

- Vergleicht **zwei verschiedene Tabellen** miteinander
- Verwendet `company_a` und `company_b` Tabellen
- Findet √ºbereinstimmende Datens√§tze zwischen den Tabellen

## Datenfelder

Die Anwendung arbeitet mit folgenden **deutschen Standardfeldern**:

- `SATZNR`: Eindeutige Satz-ID  
- `NAME`: Nachname
- `VORNAME`: Vorname
- `GEBURTSDATUM`: Geburtsdatum (YYYY-MM-DD)
- `GESCHLECHT`: Geschlecht (M/W)
- `LAND`: L√§ndercode (z.B. D)
- `POSTLEITZAHL`: Deutsche Postleitzahl
- `ORT`: Ortschaft/Stadt
- `ADRESSZEILE`: Vollst√§ndige Adresse

## Ausgabe

Die Anwendung erstellt folgende Dateien im `output/` Verzeichnis:

### Persistente DuckDB-Datei
- `splink_data.duckdb`: Hauptdatenbank mit allen Tabellen und Ergebnissen

### Kompatibilit√§tsdateien (optional)
- `predictions.csv`: Duplikat-Vorhersagen mit Wahrscheinlichkeiten (mit `--save-csv-files`)
- `target_table.csv`: Zieltabelle mit finalen Ergebnissen (mit `--save-csv-files`)

### Visualisierungen
- `comprehensive_evaluation_analysis.png`: Umfassende Analyse (6 Plots)
- `detailed_threshold_analysis.png`: Schwellenwert-Sensitivit√§tsanalyse
- `match_quality_heatmap.png`: Qualit√§ts-Heatmaps
- `match_probability_distribution.png`: Standard-Wahrscheinlichkeitsverteilung

### Testdaten
- `company_a_data.csv` / `company_b_data.csv`: Generierte Testdaten (falls generiert)
- `partner_test.csv`: Beispiel-Partnerdaten

### Evaluationsbericht
- `evaluation_report.md`: Umfassender Evaluationsbericht mit Metriken und Empfehlungen

## Splink-Konfiguration

Die Anwendung verwendet folgende optimierte Splink-Features:

- **Blocking Rules**: Kombinierte Regeln f√ºr bessere Performance
  - Einzelfelder: NAME, POSTLEITZAHL, ORT
  - Kombinationen: NAME+VORNAME, NAME+GEBURTSDATUM, PLZ+ORT
  - Soundex-basierte phonetische √Ñhnlichkeit
- **Comparison Functions**: Deutsche Datenstrukturen optimiert
  - Levenshtein-Distanz f√ºr Namen und Adressen
  - Exakte Matches f√ºr Strukturdaten (Datum, Geschlecht, PLZ)
- **EM Training**: Machine Learning f√ºr Parameter-Optimierung
- **Match Probability**: Wahrscheinlichkeitsbasierte Duplikaterkennung
- **Enhanced Evaluation**: Umfassende Statistiken und Visualisierungen

## Technische Details

- **Splink Version**: 4.x (neueste API)
- **Database Backend**: DuckDB (In-Memory)
- **Data Processing**: Pandas
- **Visualization**: Matplotlib + Seaborn
- **CLI Framework**: Click
- **Optional Dependencies**: jellyfish (f√ºr erweiterte Normalisierung)

## Fehlerbehebung

### H√§ufige Probleme

1. **"Salting partitions must be > 1"**: Bereits behoben in aktueller Version
2. **jellyfish nicht installiert**: L√§uft auch ohne, aber ohne erweiterte Features
3. **Zu wenig Speicher**: Reduzieren Sie die Datenmenge oder verwenden Sie kleinere Testdaten

### Support

Bei Fragen oder Problemen pr√ºfen Sie:
- `IMPLEMENTATION_SUMMARY.md` f√ºr technische Details
- `HYBRID_NORMALIZATION.md` f√ºr Normalisierungsdetails
- `EVALUATION_ENHANCEMENT.md` f√ºr Evaluierungsfeatures
