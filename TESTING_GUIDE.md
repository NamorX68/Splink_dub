# Schnelltest der neuen persistenten DuckDB-Features

## ğŸ§ª Testschritte

### **1. Erste Einrichtung (einmalig)**
```bash
# Testdaten generieren - erstellt die persistente DuckDB
uv run python src/dublette/app.py --generate-test-data

# PrÃ¼fen was erstellt wurde
ls -la output/
```

**Erwartung**: 
- `splink_data.duckdb` wurde erstellt
- CSV-Dateien sind weiterhin da
- Normale Duplikaterkennung lÃ¤uft

### **2. Performance-Test**
```bash
# Zeitmessung: Zweiter Lauf (sollte viel schneller sein)
time uv run python src/dublette/app.py

# Noch schneller: mit vorhandenen Ergebnissen
time uv run python src/dublette/app.py --use-existing-results
```

**Erwartung**:
- Zweiter Lauf: 70-90% schneller
- Mit `--use-existing-results`: 95% schneller
- Meldung "Using existing tables from persistent database"

### **3. Datenbankfunktionen**
```bash
# Datenbankstatistiken anzeigen
uv run python src/dublette/app.py --show-db-stats

# Alle Tabellen exportieren
uv run python src/dublette/app.py --export-db-to-csv

# Datenbank optimieren
uv run python src/dublette/app.py --cleanup-db
```

**Erwartung**:
- Ãœbersicht Ã¼ber alle Tabellen und deren GrÃ¶ÃŸe
- Export funktioniert einwandfrei
- Cleanup entfernt temporÃ¤re Tabellen

### **4. Verschiedene Modi testen**
```bash
# Multi-Table-Modus
uv run python src/dublette/app.py --multi-table --generate-test-data

# Single-Table mit existierenden Daten
uv run python src/dublette/app.py --use-existing-results

# Force Refresh
uv run python src/dublette/app.py --force-refresh
```

**Erwartung**:
- Alle Modi funktionieren mit der Datenbank
- Intelligente Erkennung ob Daten neu geladen werden mÃ¼ssen

### **5. KompatibilitÃ¤t prÃ¼fen**
```bash
# Alte Kommandos funktionieren weiterhin
uv run python src/dublette/app.py --input-file output/partner_test.csv
uv run python src/dublette/app.py --enhanced-normalization

# CSV-Dateien werden weiterhin erstellt
ls -la output/*.csv
```

**Erwartung**:
- VollstÃ¤ndige Backward Compatibility
- CSV-Dateien weiterhin verfÃ¼gbar

## ğŸ” Was zu prÃ¼fen ist

### **Dateistruktur**
```
output/
â”œâ”€â”€ splink_data.duckdb          # ğŸ†• Hauptdatenbank
â”œâ”€â”€ predictions.csv             # KompatibilitÃ¤t
â”œâ”€â”€ target_table.csv            # KompatibilitÃ¤t
â”œâ”€â”€ company_a_data.csv          # Originaldaten
â”œâ”€â”€ company_b_data.csv          # Originaldaten
â””â”€â”€ *.png                       # Plots
```

### **Logging-Ausgaben**
- `ğŸ“ Connecting to persistent DuckDB: ...`
- `âœ… Using existing tables from persistent database`
- `ğŸ’¾ Saving predictions to persistent database`
- `ğŸ“Š DATABASE STATISTICS`

### **Performance-Messungen**
- Erster Lauf: Normal (Baseline)
- Zweiter Lauf: Deutlich schneller
- Mit `--use-existing-results`: Sehr schnell

## ğŸ› HÃ¤ufige Probleme

### **"Table already exists"**
```bash
# LÃ¶sung: Force Refresh
uv run python src/dublette/app.py --force-refresh
```

### **"Database locked"**
```bash
# LÃ¶sung: Cleanup
uv run python src/dublette/app.py --cleanup-db
```

### **"No existing results found"**
```bash
# Normal: LÃ¤uft einmal vollstÃ¤ndig und speichert dann
uv run python src/dublette/app.py
```

## âœ… Erfolgskriterien

1. **Persistenz**: Daten bleiben zwischen Sessions erhalten
2. **Performance**: Deutliche Zeitersparnis bei wiederholten LÃ¤ufen
3. **KompatibilitÃ¤t**: Alle bestehenden Kommandos funktionieren
4. **FunktionalitÃ¤t**: Alle Features arbeiten mit der Datenbank
5. **FlexibilitÃ¤t**: Verschiedene Modi und Optionen verfÃ¼gbar

**Fazit**: Die persistente DuckDB-Integration funktioniert transparent und macht das System erheblich effizienter! ğŸš€
